env:
    env_name: &env_name "BipedalWalker-v3"
    record: True
    seed: 69420
<<<<<<< Updated upstream
    discrete: False
    min_expected_reward: 300
=======
    min_expected_reward: 300
    ## NEED TO PUT STATE INFO HERE
    discrete: False
>>>>>>> Stashed changes
    obs_dim: 24
    action_dim: 4

model_training:
    num_episodes_eval: 5
    record_freq: 5
    summary_freq: 1
    use_baseline: &use_baseline True
<<<<<<< Updated upstream
    normalize_advantage: False
=======
    normalize_advantage: True  
>>>>>>> Stashed changes
    device: "gpu" # cpu/gpu
    compile: False
    compile_mode: "default"

<<<<<<< Updated upstream
hyper_params:
    max_ep_len: 1600 # maximum episode length (Note: this value must be strictly less than or equal to our batch size)
    num_batches: 6000 # number of batches trained on
    batch_size: 5000 # number of steps used to compute each policy update
    learning_rate: 0.001
    gamma: 0.99 # the discount factor
    n_layers: 7
    layer_size: 256

output:
    output_path: !join &output_path ["ModelWeights/", *env_name, "-{}-"]
=======
hyper_params: 
    max_ep_len: 1600 # maximum episode length (Note: this value must be strictly less than or equal to our batch size)
    num_batches: 10000 # number of batches trained on
    batch_size: 1600 # number of steps used to compute each policy update
    learning_rate: 0.0001
    gamma: 0.99 # the discount factor
    n_layers: 2
    layer_size: 256

output:
    output_path: &output_path !join ["ModelWeights/", *env_name, "-{}-"]
>>>>>>> Stashed changes
    actor_output: !join [*output_path, "/actor.weights.pt"]
    critic_output: !join [*output_path, "/critic.weights.pt"]
    log_path: !join [*output_path, "/log.txt"]
    scores_output: !join [*output_path, "/scores.npy"]
    plot_output: !join [*output_path, "/scores.png"]
<<<<<<< Updated upstream
    record_path: !join [*output_path, "/VIDYA"]
=======
    record_path: !join [*output_path, "/VIDYA"]
>>>>>>> Stashed changes
