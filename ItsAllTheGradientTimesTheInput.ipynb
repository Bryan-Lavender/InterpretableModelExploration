{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f2cd0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    }
   ],
   "source": [
    "from Explanations_Models.Custom_DT.LIME import LIME\n",
    "from FeatureImportance.FI import FeatureImportance\n",
    "import yaml\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import unittest\n",
    "from DeepLearning_Models.utils.general import join, plot_combined\n",
    "from DeepLearning_Models.ActorCritic.policy_gradient import PolicyGradient\n",
    "import json\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from EnvRunner import GymRunner\n",
    "\n",
    "\n",
    "# Suppress all deprecation warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "yaml.add_constructor(\"!join\", join)\n",
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "# parser.add_argument(\"--config_filename\", required=False, type=str)\n",
    "domain = \"cartpole\"\n",
    "config_file = open(\"config_envs/{}.yml\".format(domain))\n",
    "config = yaml.load(config_file, Loader=yaml.FullLoader)\n",
    "config.update(yaml.load(open(\"config_explanations/{}.yml\".format(\"cartpole\"), encoding=\"utf8\"), Loader= yaml.FullLoader))\n",
    "Runner = GymRunner(config)\n",
    "Runner.load_weights(PATH = None)\n",
    "model = Runner.model.policy.network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7cc5ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient(model, point):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the model output with respect to the input point.\n",
    "    \n",
    "    Args:\n",
    "        model: a PyTorch model\n",
    "        point: a 1D torch tensor of shape (input_dim,) or (1, input_dim)\n",
    "\n",
    "    Returns:\n",
    "        gradient: a 1D torch tensor of shape (input_dim,)\n",
    "    \"\"\"\n",
    "    point = torch.tensor(point, dtype=torch.float32)\n",
    "    model.eval()\n",
    "    point = point.clone().detach().requires_grad_(True)\n",
    "\n",
    "    output = model(point)\n",
    "    # if output.ndim > 1:\n",
    "    #     output = output.squeeze()\n",
    "\n",
    "    # If scalar output\n",
    "    #assert output.numel() == 1, \"Output must be scalar for gradient calculation\"\n",
    "    output.backward()\n",
    "\n",
    "    gradient = point.grad.detach().clone()\n",
    "    return gradient\n",
    "\n",
    "def smooth_grad(model, input_tensor, n_samples=50, noise_std=2., multiply_by_input=False):\n",
    "    \"\"\"\n",
    "    Computes SmoothGrad explanation by averaging noisy gradients.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model (should be in eval mode)\n",
    "        input_tensor: 1D torch.Tensor of shape [input_dim]\n",
    "        n_samples: number of noisy samples to draw\n",
    "        noise_std: standard deviation of Gaussian noise to add\n",
    "        multiply_by_input: if True, returns gradient * input (Gradient Ã— Input)\n",
    "\n",
    "    Returns:\n",
    "        smooth_grad: torch.Tensor of shape [input_dim]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    grads = []\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        # Add noise to input\n",
    "        noisy_input = input_tensor.clone().detach() + torch.randn_like(input_tensor) * noise_std\n",
    "        noisy_input.requires_grad_(True)\n",
    "\n",
    "        # Forward and backward\n",
    "        output = model(noisy_input.unsqueeze(0))  # batch dim\n",
    "        \n",
    "        output.backward()\n",
    "\n",
    "        grad = noisy_input.grad.clone().detach()\n",
    "        \n",
    "        if multiply_by_input:\n",
    "            grad = grad * noisy_input.detach()\n",
    "\n",
    "        grads.append(grad)\n",
    "\n",
    "    # Average all gradients\n",
    "    return torch.stack(grads).mean(dim=0)\n",
    "\n",
    "def grad_cam_ish(model, point):\n",
    "    activations = {}\n",
    "    gradients = {}\n",
    "    def forward_hook(name):\n",
    "        def hook(module, input, output):\n",
    "            activations[name] = output.detach()\n",
    "        return hook\n",
    "\n",
    "    # Hook to capture gradients w.r.t. outputs\n",
    "    def backward_hook(name):\n",
    "        def hook(module, grad_input, grad_output):\n",
    "            gradients[name] = grad_output[0].detach()\n",
    "        return hook\n",
    "\n",
    "    # Register hooks for each layer\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.register_forward_hook(forward_hook(name))\n",
    "            module.register_backward_hook(backward_hook(name))\n",
    "\n",
    "    # === Sample input ===\n",
    "    input_point = torch.tensor(point, requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(input_point)\n",
    "\n",
    "    # Backward pass (ensure scalar output)\n",
    "    output.backward()\n",
    "\n",
    "    # Mean gradient for each neuron (shape: [32])\n",
    "    weights = gradients['input_layer'].mean(dim=0)  # like alpha_k\n",
    "\n",
    "    # Weighted sum of activations (shape: [1, 32])\n",
    "    cam = (weights * activations['input_layer'].squeeze()).cpu().numpy()\n",
    "\n",
    "    # Optional: ReLU to zero out negatives\n",
    "    cam = np.maximum(cam, 0)\n",
    "\n",
    "    input_contrib = cam @ model.input_layer.weight.detach().numpy()\n",
    "    return input_contrib\n",
    "\n",
    "def integrated_gradients(model, input_tensor, baseline=None, steps=50):\n",
    "    \"\"\"\n",
    "    Compute Integrated Gradients for a single input.\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        input_tensor: 1D torch.Tensor, input point\n",
    "        baseline: baseline input (same shape), default zero\n",
    "        steps: number of interpolation steps\n",
    "\n",
    "    Returns:\n",
    "        IG attribution vector\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if baseline is None:\n",
    "        baseline = torch.zeros_like(input_tensor)\n",
    "\n",
    "    scaled_inputs = [\n",
    "        baseline + (float(i) / steps) * (input_tensor - baseline)\n",
    "        for i in range(1, steps + 1)\n",
    "    ]\n",
    "\n",
    "    grads = []\n",
    "    for x in scaled_inputs:\n",
    "        x = x.clone().detach().requires_grad_(True)\n",
    "        output = model(x.unsqueeze(0))\n",
    "        output.backward()\n",
    "        grads.append(x.grad.detach())\n",
    "\n",
    "    avg_grad = torch.stack(grads).mean(dim=0)\n",
    "    ig = (input_tensor - baseline) * avg_grad\n",
    "    return ig\n",
    "\n",
    "from FeatureImportance.methods.FiniteDifferences import FiniteDifferences\n",
    "from FeatureImportance.methods.LRP import TwoDWeights_LRPModel\n",
    "from FeatureImportance.methods.LIME import LIME\n",
    "from FeatureImportance.methods.SHAP import SHAP\n",
    "\n",
    "lime = LIME(model, 1)\n",
    "shap = SHAP(model)\n",
    "fd = FiniteDifferences(model)\n",
    "lrp = TwoDWeights_LRPModel(model)\n",
    "model = model.to(\"cpu\")\n",
    "def get_LRP(model, points):\n",
    "    lrp_exp = []\n",
    "    POEs_torch = torch.tensor(points, dtype = torch.float32)\n",
    "    for i in POEs_torch:\n",
    "        lrp = TwoDWeights_LRPModel(model)\n",
    "        _, lrp_expt = lrp.get_FI(i)\n",
    "        \n",
    "        lrp_exp.append(torch.stack(lrp_expt).numpy())\n",
    "    lrp_exp = np.stack(lrp_exp)\n",
    "    return lrp_exp\n",
    "\n",
    "def get_grad(model, points):\n",
    "    torch.set_grad_enabled(True)\n",
    "    x = torch.tensor(points, dtype = torch.float32, requires_grad=True).to()\n",
    "    model = model.requires_grad_(True)\n",
    "    y = model(x)  # shape: [1, 3]\n",
    "    \n",
    "    jacobian = torch.zeros(points.shape[0], config[\"env\"][\"action_dim\"], points.shape[1])\n",
    "\n",
    "    for i in range(config[\"env\"][\"action_dim\"]):\n",
    "        # Create grad_outputs with 1 in i-th dimension, zeros elsewhere\n",
    "        grad_outputs = torch.zeros_like(y)\n",
    "        grad_outputs[:, i] = 1.0\n",
    "\n",
    "        # Compute gradients\n",
    "        grads = torch.autograd.grad(\n",
    "            outputs=y,\n",
    "            inputs=x,\n",
    "            grad_outputs=grad_outputs,\n",
    "            retain_graph=True,\n",
    "            create_graph=True,\n",
    "            allow_unused=True\n",
    "        )[0]  # shape: [batch_size, input_dim]\n",
    "\n",
    "    jacobian[:, i, :] = grads\n",
    "    return jacobian\n",
    "\n",
    "def get_SHAP(model, points):\n",
    "    shap = SHAP(model)\n",
    "    shap = shap.get_fi(points)[1]\n",
    "    return shap\n",
    "\n",
    "def get_LIME(model, points):\n",
    "    lime = LIME(model, 2)\n",
    "    _, lime = lime.get_fi(points)\n",
    "    return lime\n",
    "\n",
    "def get_SmoothGrad(model, points, multiply_by_input= False):\n",
    "    points = torch.tensor(points, dtype = torch.float32)\n",
    "    exps = []\n",
    "    for i in points:\n",
    "        exp = smooth_grad(model, i, multiply_by_input=multiply_by_input)\n",
    "        exps.append(exp.detach().numpy())\n",
    "    return np.stack(exps)\n",
    "\n",
    "def get_GradCam(model, points):\n",
    "    points = torch.tensor(points, dtype = torch.float32)\n",
    "    exps = []\n",
    "    for i in points:\n",
    "        exp = grad_cam_ish(model, i)\n",
    "        exps.append(exp)\n",
    "    return np.stack(exps)\n",
    "\n",
    "def get_IntegratedGradients(model, points):\n",
    "    points = torch.tensor(points, dtype = torch.float32)\n",
    "    exps = []\n",
    "    for i in points:\n",
    "        exp = integrated_gradients(model, i)\n",
    "        exps.append(exp.detach().numpy())\n",
    "    return np.stack(exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a52f10",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m POEs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m4.0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4.0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4.0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4.0\u001b[39m],[\u001b[38;5;241m1.0\u001b[39m,\u001b[38;5;241m1.0\u001b[39m,\u001b[38;5;241m1.0\u001b[39m,\u001b[38;5;241m1.0\u001b[39m],[\u001b[38;5;241m0.0\u001b[39m,\u001b[38;5;241m0.0\u001b[39m,\u001b[38;5;241m0.0\u001b[39m,\u001b[38;5;241m0.0\u001b[39m]])\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# print(\"FD: \", get_grad(model, POEs))\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# print(\"LRP: \", get_LRP(model, POEs))\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# shapthing = get_SHAP(model, POEs)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# print(\"LIME: \", get_LIME(model, POEs))\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmooth_grad: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mget_SmoothGrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPOEs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[41], line 198\u001b[0m, in \u001b[0;36mget_SmoothGrad\u001b[1;34m(model, points, multiply_by_input)\u001b[0m\n\u001b[0;32m    196\u001b[0m exps \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m points:\n\u001b[1;32m--> 198\u001b[0m     exp \u001b[38;5;241m=\u001b[39m \u001b[43msmooth_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiply_by_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiply_by_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m     exps\u001b[38;5;241m.\u001b[39mappend(exp\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mstack(exps)\n",
      "Cell \u001b[1;32mIn[41], line 52\u001b[0m, in \u001b[0;36msmooth_grad\u001b[1;34m(model, input_tensor, n_samples, noise_std, multiply_by_input)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Forward and backward\u001b[39;00m\n\u001b[0;32m     50\u001b[0m output \u001b[38;5;241m=\u001b[39m model(noisy_input\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))  \u001b[38;5;66;03m# batch dim\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m grad \u001b[38;5;241m=\u001b[39m noisy_input\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multiply_by_input:\n",
      "File \u001b[1;32mc:\\Users\\Bryan Lavender\\.conda\\envs\\XAI_DT_WBox2d\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bryan Lavender\\.conda\\envs\\XAI_DT_WBox2d\\lib\\site-packages\\torch\\autograd\\__init__.py:282\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    273\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    274\u001b[0m     (inputs,)\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[0;32m    279\u001b[0m )\n\u001b[0;32m    281\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[1;32m--> 282\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[1;32mc:\\Users\\Bryan Lavender\\.conda\\envs\\XAI_DT_WBox2d\\lib\\site-packages\\torch\\autograd\\__init__.py:151\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 151\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    152\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m         )\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[0;32m    155\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    156\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m         )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "POEs = np.array([[4.0, -4.0, -4.0,-4.0],[1.0,1.0,1.0,1.0],[0.0,0.0,0.0,0.0]])\n",
    "# print(\"FD: \", get_grad(model, POEs))\n",
    "# print(\"LRP: \", get_LRP(model, POEs))\n",
    "# shapthing = get_SHAP(model, POEs)\n",
    "# print(\"LIME: \", get_LIME(model, POEs))\n",
    "# print(\"smooth_grad: \", get_SmoothGrad(model, POEs))\n",
    "# print(\"grad_cam: \", get_GradCam(model, POEs))\n",
    "# print(\"integrated_grads: \", get_GradCam(model, POEs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33b53be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
